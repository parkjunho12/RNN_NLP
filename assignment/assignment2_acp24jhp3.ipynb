{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM4513/6513] Assignment: Topic Classification with a Feedforward Network\n",
    "\n",
    "\n",
    "### Instructor: Nikos Aletras, Nafise Sadat Moosavi\n",
    "\n",
    "\n",
    "\n",
    "**Objective:** This assignment aims to develop a Feedforward Neural Network for classifying news articles into three topics: *Politics*, *Sports*, and *Economy*.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "The data you will use for the task is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "Class 1: Politics, Class 2: Sports, Class 3: Economy\n",
    "\n",
    "* `data_topic/train.csv`: 2,400 articles (800 per class) for training.\n",
    "* `data_topic/dev.csv`: 150 articles (50 per class) for hyperparameter tuning and validation.\n",
    "* `data_topic/test.csv`: 900 articles (300 per class) for final evaluation.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "*  **Text Preprocessing (1 mark):**\n",
    "    * Implement methods to convert raw text into input vectors suitable for your neural network.\n",
    "   \n",
    "\n",
    "*  **Feedforward Neural Network Implementation (3 marks):** The network consists of the following components:\n",
    "    * **Input Layer:** Implement a one-hot encoding followed by an Embedding weight matrix. (**1 mark**)\n",
    "    * **Hidden Layer:** Calculate the mean embedding vector of all words in the input, and apply a ReLU activation function. (**1 mark**)\n",
    "    * **Output Layer:** Implement a softmax activation function for multi-class classification. (**1 mark**)\n",
    "    \n",
    "\n",
    "*  **Training with Stochastic Gradient Descent (SGD) (6 marks):**\n",
    "    * Implement the forward pass to compute intermediate outputs. (**2 marks**)\n",
    "    * Implement the backward pass to compute gradients and update weights. (**3 marks**)\n",
    "    * Implement and use the Categorical Cross-entropy loss as the objective function. (**1 mark**)\n",
    "    * Implement and apply dropout regularization after the hidden layer. (**1 mark**)\n",
    "\n",
    "*  **Hyperparameter Tuning (2 marks):**\n",
    "    * Systematically tune the following hyperparameters:\n",
    "        * Learning rate (suggest small values).\n",
    "        * Embedding size (e.g. 50, 300, 500).\n",
    "        * Dropout rate (e.g. 0.2, 0.5).\n",
    "    * Present training and validation performance (loss and accuracy) for different hyperparameter combinations using tables or graphs.\n",
    "    \n",
    "\n",
    "*  **Pre-trained Embeddings (GloVe) (1 mark)**:\n",
    "    * Re-train your neural network using pre-trained GloVe embeddings (300 dimensions) from Common Crawl.\n",
    "    * Initialize the embedding layer with pre-trained weights.\n",
    "    * Freeze the embedding weights during training (do not update them).\n",
    "    * Repeat hyperparameter tuning and learning process analysis.\n",
    "    * Compare performance with the model using randomly initialized embeddings.\n",
    "\n",
    "*  **Network Extension (3 marks):**\n",
    "    * Extend the feedforward network by adding one or two additional hidden layers.\n",
    "    * Repeat hyperparameter tuning (using a representative subset of combinations).\n",
    "    * Analyze and discuss the impact of adding more hidden layers on performance.\n",
    "\n",
    "    \n",
    "*  **Analysis and Discussion of Results (12 marks):**\n",
    "    * Justify briefly and concisely all design choices, e.g. text processing, hyperparameters, network architecture. (**2 marks*) \n",
    "    * Plot the training and validation loss curves across epochs for all models (excluding the hyperparameter tuning phase). Analyze the learning curves to determine if the model is overfitting, underfitting, or well-fitted. (**2 marks**)\n",
    "    * Provide a detailed analysis of results, including:\n",
    "        * Explanations for performance differences between models. (**3 marks**)\n",
    "        * Error analysis, showing examples and discussion of misclassifications. (**3 marks**)\n",
    "    * Please provide short and concise justifications.\n",
    "\n",
    "*  **Code Documentation and Efficiency (3 marks):**\n",
    "    * Provide well-documented and commented code. (1 mark)\n",
    "    * Implement efficient solutions using NumPy arrays where possible, ensuring the notebook executes within 10 minutes, excluding hyperparameter tuning and loading pre-trained vectors, on a standard computer (e.g. Intel Core i5, 8/16GB RAM). (2 marks)\n",
    "    * Refer to Lab 1 for efficiency tips.\n",
    "\n",
    "**Pre-trained Embeddings:**\n",
    "\n",
    "* Download GloVe embeddings (glove.840B.300d.zip) from [http://nlp.stanford.edu/data/glove.840B.300d.zip](http://nlp.stanford.edu/data/glove.840B.300d.zip).\n",
    "\n",
    "**Memory Management:**\n",
    "\n",
    "* Use `del W` and `gc.collect()` to free memory after each experiment.\n",
    "\n",
    "**Memory Management:**\n",
    "\n",
    "* Use `del W` and `gc.collect()` to free memory after each experiment.\n",
    "\n",
    "**Note:** \n",
    "This assignment is designed to be challenging and requires a strong understanding of the underlying mathematical concepts introduced in the class beyond lecture slides. Students are encouraged to consult Chapters 4, 5, 6 and 7 from the [course textbook](https://web.stanford.edu/~jurafsky/slp3/). Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "You **must** submit a Jupyter Notebook file (assignment_yourusername.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`, you need to have a Latex distribution installed e.g. MikTex or MacTex and pandoc). If you are unable to export the pdf via Latex, you can print the notebook web page to a pdf file from your browser (e.g. on Firefox: File->Print->Save to PDF). Failure to submit both the .ipynb and pdf files will result in mark deductions.\n",
    "\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any additional functionality from the [Python Standard Library](https://docs.python.org/3/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are **not allowed to use any third-party library** such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras, Pytorch etc.. You should mention if you've used Windows to write and test your code because we mostly use Unix based machines for marking (e.g. Ubuntu, MacOS). \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results and discussion is as important as the implementation and accuracy of your models. Please be brief and consice in your discussion and analyses. \n",
    "\n",
    "This assignment will be marked out of 30. It is worth 30\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **15:00 on Thursday, 3 Apr 2025** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade. Use of GenAI is not permitted for this assignment and will be considered as use of unfair means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "import zipfile\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   0                                                  1\n",
      "0  1  Reuters - Venezuelans turned out early\\and in ...\n",
      "1  1  Reuters - South Korean police used water canno...\n",
      "2  1  Reuters - Thousands of Palestinian\\prisoners i...\n",
      "3  1  AFP - Sporadic gunfire and shelling took place...\n",
      "4  1  AP - Dozens of Rwandan soldiers flew into Suda...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./data_topic/train.csv', header=None)\n",
    "dev_data = pd.read_csv('./data_topic/dev.csv', header=None)\n",
    "test_data = pd.read_csv('./data_topic/test.csv', header=None)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.753874Z",
     "start_time": "2020-04-02T14:26:39.749647Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:23:17.181553Z",
     "start_time": "2020-05-11T08:23:17.178314Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "                   stop_words=[], vocab=set()):\n",
    "    # Step 1: Tokenize using the pattern\n",
    "    tokens = re.findall(token_pattern, x_raw.lower())\n",
    "\n",
    "    # Step 2: Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Step 3: Exract ngrams within the given range\n",
    "    min_n, max_n = ngram_range\n",
    "    ngrams = []\n",
    "\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i:i + n]) \n",
    "\n",
    "            if (not vocab) or (ngram in vocab):\n",
    "                ngrams.append(ngram)\n",
    "            \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency (i.e. number of documents that an ngram appears) as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocab(X_raw, ngram_range=(1,2), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[]):\n",
    "    df = Counter()\n",
    "    ngram_counts = Counter()\n",
    "\n",
    "    # Step 1: Process each document to collect ngram counts and document frequencies\n",
    "    for doc in X_raw:\n",
    "        ngrams_in_doc = extract_ngrams(doc, ngram_range=ngram_range, token_pattern=token_pattern,\n",
    "                                       stop_words=stop_words)\n",
    "        used_ngarm = []\n",
    "  \n",
    "        for ngram in ngrams_in_doc:\n",
    "            ngram_counts[ngram] += 1\n",
    "\n",
    "            if ngram not in used_ngarm:\n",
    "                used_ngarm.append(ngram)\n",
    "                df[ngram] += 1\n",
    "\n",
    "\n",
    "    # Step 2: Apply document frequency filter {min_df}\n",
    "    vocab = {ngram for ngram, freq in df.items() if df[ngram] >= min_df}\n",
    "    \n",
    "    # Step 3: Apply top-N frequency filter {keep_topN}\n",
    "    top_ngrams = [ngram for ngram, _ in ngram_counts.most_common(keep_topN)]\n",
    "    vocab = set(top_ngrams if keep_topN >0 else ngram_counts)\n",
    "    \n",
    "    \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'after',\n",
       " 'ap',\n",
       " 'athens',\n",
       " 'first',\n",
       " 'monday',\n",
       " 'new',\n",
       " 'new york',\n",
       " 'olympic',\n",
       " 'reuters',\n",
       " 'said',\n",
       " 'tuesday',\n",
       " 'two',\n",
       " 'us',\n",
       " 'wednesday',\n",
       " 'york'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs, df, ngram_counts = get_vocab(\n",
    "        X_raw=train_data[1], min_df=1, keep_topN=15, stop_words=stop_words\n",
    "    )\n",
    "\n",
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> vocabulary id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_word_index_mappings(vocabs):\n",
    "    all_tokens = set()\n",
    "\n",
    "    for vocab in vocabs:\n",
    "        all_tokens.add(vocab)\n",
    "\n",
    "    word_to_id = {word: idx for idx, word in enumerate(all_tokens)}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.047887Z",
     "start_time": "2020-04-02T14:26:44.920631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_docs = train_data[1]\n",
    "test_docs = test_data[1]\n",
    "dev_docs = dev_data[1]\n",
    "\n",
    "train_vocabs, train_df, train_ngram_counts = get_vocab(\n",
    "        X_raw=train_docs, min_df=5, keep_topN=50000, stop_words=stop_words\n",
    "    )\n",
    "test_vocabs, test_df, test_ngram_counts = get_vocab(\n",
    "        X_raw=test_docs, min_df=5, keep_topN=50000, stop_words=stop_words\n",
    "    )\n",
    "dev_vocabs, dev_df, dev_ngram_counts = get_vocab(\n",
    "        X_raw=dev_docs, min_df=5, keep_topN=50000, stop_words=stop_words\n",
    "    )\n",
    "\n",
    "all_vocabs = set()\n",
    "for vocab in train_vocabs:\n",
    "    all_vocabs.add(vocab)\n",
    "for vocab in test_vocabs:\n",
    "    all_vocabs.add(vocab)\n",
    "for vocab in dev_vocabs:\n",
    "    all_vocabs.add(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.752658Z",
     "start_time": "2020-04-02T14:26:45.730409Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reuters', 'venezuelans', 'turned', 'out', 'early', 'large', 'numbers', 'sunday', 'vote', 'historic', 'referendum', 'either', 'remove', 'left', 'wing', 'president', 'hugo', 'chavez', 'office', 'give', 'him', 'new', 'mandate', 'govern', 'next', 'two', 'years', 'reuters venezuelans', 'venezuelans turned', 'turned out', 'out early', 'early large', 'large numbers', 'numbers sunday', 'sunday vote', 'vote historic', 'historic referendum', 'referendum either', 'either remove', 'remove left', 'left wing', 'wing president', 'president hugo', 'hugo chavez', 'chavez office', 'office give', 'give him', 'him new', 'new mandate', 'mandate govern', 'govern next', 'next two', 'two years', 'reuters venezuelans turned', 'venezuelans turned out', 'turned out early', 'out early large', 'early large numbers', 'large numbers sunday', 'numbers sunday vote', 'sunday vote historic', 'vote historic referendum', 'historic referendum either', 'referendum either remove', 'either remove left', 'remove left wing', 'left wing president', 'wing president hugo', 'president hugo chavez', 'hugo chavez office', 'chavez office give', 'office give him', 'give him new', 'him new mandate', 'new mandate govern', 'mandate govern next', 'govern next two', 'next two years']\n"
     ]
    }
   ],
   "source": [
    "all_word_to_id, all_id_to_word = convert_word_index_mappings(all_vocabs)\n",
    "\n",
    "train_ngrams = [extract_ngrams(doc, stop_words=stop_words) for doc in train_docs]\n",
    "test_ngrams = [extract_ngrams(doc, stop_words=stop_words) for doc in test_docs]\n",
    "dev_ngrams = [extract_ngrams(doc, stop_words=stop_words) for doc in dev_docs]\n",
    "\n",
    "\n",
    "def convert_to_indices(doc, word_to_index):\n",
    "    indices = []\n",
    "    for ngram in doc:\n",
    "        ngram_indice = []\n",
    "        for vocab in ngram:\n",
    "            if vocab in word_to_index:\n",
    "                ngram_indice.append(word_to_index[vocab])\n",
    "        indices.append(ngram_indice)\n",
    "    return indices\n",
    "\n",
    "X_train_indices = convert_to_indices(train_ngrams, all_word_to_id)\n",
    "X_test_indices = convert_to_indices(test_ngrams, all_word_to_id)\n",
    "X_dev_indices = convert_to_indices(dev_ngrams, all_word_to_id)\n",
    "\n",
    "print(train_ngrams[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_array = np.array(train_data[0])\n",
    "Y_test_array = np.array(test_data[0])\n",
    "Y_dev_array = np.array(dev_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers. Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:41:20.918617Z",
     "start_time": "2020-04-02T15:41:20.915597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=all_vocab.size, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.1):\n",
    "    W = {}\n",
    "\n",
    "    # Initialize embedding matrix (vocab_size, embedding_dim)\n",
    "    W[0] = np.random.uniform(-init_val, init_val, (vocab_size, embedding_dim)).astype(np.float32)\n",
    "\n",
    "    # Layer sizes (embedding_dim → hidden layers → output)\n",
    "    layer_sizes = [embedding_dim] + hidden_dim + [num_classes]\n",
    "\n",
    "    # Initialize weight matrices for hidden and output layers\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        W[i] = np.random.uniform(-init_val, init_val, (layer_sizes[i-1], layer_sizes[i])).astype(np.float32)\n",
    "\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[ 0.03929384, -0.04277213, -0.05462971, ...,  0.06921097,\n",
       "         -0.0752154 ,  0.01929738],\n",
       "        [-0.0967215 ,  0.04423687, -0.09845249, ...,  0.00988261,\n",
       "         -0.09449141, -0.0936164 ],\n",
       "        [ 0.04027196,  0.04151622,  0.09198783, ...,  0.06915964,\n",
       "         -0.02326545, -0.08785208],\n",
       "        ...,\n",
       "        [-0.04425354, -0.08308988,  0.07742252, ...,  0.01545605,\n",
       "         -0.08740567, -0.08509608],\n",
       "        [ 0.06906886, -0.00988667, -0.07630441, ...,  0.07359806,\n",
       "          0.01090446, -0.05046375],\n",
       "        [ 0.05363254,  0.09605809, -0.01189931, ...,  0.09286061,\n",
       "          0.00939131,  0.00072314]], dtype=float32),\n",
       " 1: array([[ 3.94012108e-02,  1.12006348e-03],\n",
       "        [ 2.60030888e-02,  8.48027989e-02],\n",
       "        [ 4.77948003e-02,  1.84607990e-02],\n",
       "        [ 3.08759306e-02,  3.67130116e-02],\n",
       "        [ 6.88309222e-02,  3.38236056e-02],\n",
       "        [ 7.68840536e-02, -8.02739151e-03],\n",
       "        [-8.69960263e-02,  7.47316405e-02],\n",
       "        [ 6.44678399e-02, -6.07313365e-02],\n",
       "        [-9.08605084e-02, -4.62819524e-02],\n",
       "        [-5.38272262e-02, -9.82139707e-02],\n",
       "        [ 1.99470278e-02, -4.11613323e-02],\n",
       "        [ 9.34376568e-02,  1.48085607e-02],\n",
       "        [-7.86746144e-02,  5.23623405e-03],\n",
       "        [-5.29252440e-02, -3.40811275e-02],\n",
       "        [-3.39886174e-04, -9.48687419e-02],\n",
       "        [ 3.13715637e-02,  1.13442428e-02],\n",
       "        [-2.41191685e-02,  2.53747348e-02],\n",
       "        [-8.42951387e-02, -3.15424241e-02],\n",
       "        [-8.16331357e-02,  4.96547297e-03],\n",
       "        [-7.20245242e-02,  7.99671039e-02],\n",
       "        [ 1.07897995e-02, -1.95870511e-02],\n",
       "        [ 5.37298694e-02,  9.59440395e-02],\n",
       "        [-8.52864310e-02,  7.15000778e-02],\n",
       "        [-8.36008936e-02,  3.27075459e-02],\n",
       "        [ 3.99251580e-02,  9.13593620e-02],\n",
       "        [-4.39113043e-02,  4.62997332e-02],\n",
       "        [-2.76740771e-02,  8.39851424e-02],\n",
       "        [ 6.53900728e-02,  3.29835638e-02],\n",
       "        [ 8.20146725e-02,  8.16176310e-02],\n",
       "        [ 2.10366547e-02, -2.22425181e-02],\n",
       "        [-4.76795323e-02,  7.35791028e-02],\n",
       "        [ 9.67708454e-02, -1.31340940e-02],\n",
       "        [ 7.94868693e-02, -2.13013142e-02],\n",
       "        [ 6.37179147e-03,  6.90771490e-02],\n",
       "        [-8.22800025e-02,  2.63580773e-02],\n",
       "        [ 3.74332033e-02,  2.81635411e-02],\n",
       "        [-7.50442222e-02, -6.89082891e-02],\n",
       "        [ 4.56088297e-02,  5.00212200e-02],\n",
       "        [-9.96897649e-03, -2.99490727e-02],\n",
       "        [ 4.44052108e-02,  5.99643663e-02],\n",
       "        [ 6.81657298e-03,  7.15597067e-03],\n",
       "        [-6.34514466e-02,  9.68284607e-02],\n",
       "        [-9.74692628e-02,  5.63097820e-02],\n",
       "        [-4.54826690e-02, -4.96511720e-02],\n",
       "        [-2.36751828e-02, -6.14777282e-02],\n",
       "        [ 3.96035872e-02, -2.76710209e-03],\n",
       "        [ 9.88383442e-02,  4.14512083e-02],\n",
       "        [-2.35505346e-02, -4.40938659e-02],\n",
       "        [ 7.82337040e-02, -5.58327101e-02],\n",
       "        [-2.75769681e-02,  9.08640251e-02],\n",
       "        [-6.97657093e-02,  1.46476440e-02],\n",
       "        [-3.50460410e-02, -4.70502004e-02],\n",
       "        [-6.29183948e-02,  6.18702583e-02],\n",
       "        [ 6.87480643e-02,  2.00257562e-02],\n",
       "        [ 8.17761943e-02, -8.88522267e-02],\n",
       "        [-8.67120251e-02,  6.91655651e-02],\n",
       "        [-1.70356091e-02, -6.65847436e-02],\n",
       "        [-1.03975339e-02,  1.48509128e-03],\n",
       "        [-1.01664532e-02, -6.11791871e-02],\n",
       "        [ 7.74180442e-02,  9.98335774e-04],\n",
       "        [-4.47384566e-02,  6.76110163e-02],\n",
       "        [-6.87185675e-02, -3.31178047e-02],\n",
       "        [-3.71420085e-02,  2.29442492e-02],\n",
       "        [-9.67468843e-02,  4.41951118e-02],\n",
       "        [ 4.51987125e-02,  8.52840766e-02],\n",
       "        [-4.78734188e-02,  5.53729236e-02],\n",
       "        [-9.79328081e-02,  3.70692983e-02],\n",
       "        [-5.66641651e-02, -4.22490798e-02],\n",
       "        [-2.50408472e-03,  5.05597927e-02],\n",
       "        [ 2.11795568e-02, -4.07963917e-02],\n",
       "        [-7.37275481e-02,  8.44578594e-02],\n",
       "        [ 4.61718952e-03,  4.67955694e-03],\n",
       "        [ 7.17902854e-02,  1.26118874e-02],\n",
       "        [-7.00677708e-02,  9.55502912e-02],\n",
       "        [ 3.09097487e-02,  8.54611099e-02],\n",
       "        [-1.01359328e-03,  4.13832664e-02],\n",
       "        [-2.08814796e-02, -4.47678287e-03],\n",
       "        [ 9.10082459e-02, -7.71677718e-02],\n",
       "        [-5.86917205e-03,  4.14624810e-02],\n",
       "        [ 1.48637109e-02, -4.30696979e-02],\n",
       "        [ 4.89468612e-02, -2.13130121e-03],\n",
       "        [ 3.17401066e-02,  5.73463552e-02],\n",
       "        [ 2.21300870e-02,  5.69535308e-02],\n",
       "        [ 9.78310555e-02, -5.59056439e-02],\n",
       "        [ 5.53166233e-02,  5.91326728e-02],\n",
       "        [-9.51937661e-02, -7.96268214e-05],\n",
       "        [-1.49737876e-02,  9.44915041e-02],\n",
       "        [ 7.50391036e-02, -6.06343858e-02],\n",
       "        [-3.57860997e-02,  7.00978097e-03],\n",
       "        [-2.18709614e-02, -2.53389217e-02],\n",
       "        [ 7.90658370e-02,  9.07177757e-03],\n",
       "        [-1.23263951e-02,  5.23774512e-02],\n",
       "        [-2.31806170e-02,  1.29381544e-03],\n",
       "        [-1.34015186e-02,  5.24360873e-02],\n",
       "        [-2.56830435e-02, -1.58865899e-02],\n",
       "        [-3.37666948e-04,  3.98591673e-03],\n",
       "        [ 5.46297319e-02, -8.60576555e-02],\n",
       "        [ 7.98734184e-03, -5.48645519e-02],\n",
       "        [ 2.47240514e-02, -1.73642505e-02],\n",
       "        [ 7.03511164e-02,  7.02510849e-02],\n",
       "        [ 6.28250614e-02,  2.29709689e-02],\n",
       "        [-2.18640873e-03,  7.80248567e-02],\n",
       "        [-7.74009004e-02, -2.30731908e-02],\n",
       "        [-1.22658079e-02,  1.66020170e-02],\n",
       "        [ 7.99159482e-02, -9.66340601e-02],\n",
       "        [-9.19521004e-02, -2.05366388e-02],\n",
       "        [-7.42188096e-02, -9.62378606e-02],\n",
       "        [-6.05040742e-03, -6.58606887e-02],\n",
       "        [ 5.81859387e-02,  8.18017945e-02],\n",
       "        [ 7.28083104e-02, -5.45771122e-02],\n",
       "        [ 3.98026174e-03,  8.15751925e-02],\n",
       "        [-6.22615516e-02,  4.57923636e-02],\n",
       "        [ 9.90239158e-02, -4.27503176e-02],\n",
       "        [-2.41579791e-03,  3.50813940e-02],\n",
       "        [-7.21044987e-02,  9.66040045e-02],\n",
       "        [-9.76771936e-02, -9.31460336e-02],\n",
       "        [-4.48910221e-02, -3.03023569e-02],\n",
       "        [ 9.60204154e-02,  3.02206911e-02],\n",
       "        [ 2.84697190e-02, -2.08045412e-02],\n",
       "        [ 1.35437092e-02, -7.02609047e-02],\n",
       "        [ 9.85979438e-02,  7.38262087e-02],\n",
       "        [ 9.91527513e-02,  5.91252968e-02],\n",
       "        [ 4.14872915e-02, -7.69743025e-02],\n",
       "        [ 7.89583772e-02, -3.96694168e-02],\n",
       "        [ 3.61605361e-02,  3.89597602e-02],\n",
       "        [-3.18856537e-02,  6.63819015e-02],\n",
       "        [ 5.88298105e-02, -1.05550082e-03],\n",
       "        [-7.59185106e-02, -6.31311089e-02],\n",
       "        [ 1.22642312e-02,  9.83964279e-02],\n",
       "        [-7.56754279e-02,  9.78345051e-03],\n",
       "        [ 8.21307227e-02, -7.17752576e-02],\n",
       "        [-7.77492151e-02,  6.91374019e-02],\n",
       "        [-7.04160482e-02,  7.55310133e-02],\n",
       "        [-2.70366818e-02, -3.20926905e-02],\n",
       "        [ 2.67536771e-02,  1.50995580e-02],\n",
       "        [-7.44567513e-02, -8.27138778e-03],\n",
       "        [ 3.29489224e-02, -4.99243736e-02],\n",
       "        [-5.88715822e-02, -5.70695149e-03],\n",
       "        [-6.05423003e-02,  5.08802608e-02],\n",
       "        [-8.45346302e-02, -8.27770159e-02],\n",
       "        [-2.07392164e-02, -8.48026425e-02],\n",
       "        [-5.26002981e-02, -2.41366439e-02],\n",
       "        [-2.95440294e-02, -1.84980905e-04],\n",
       "        [ 8.56211036e-03, -2.07545962e-02],\n",
       "        [ 9.52915028e-02,  8.17021132e-02],\n",
       "        [ 6.31390810e-02,  3.21704359e-03],\n",
       "        [-1.98349217e-03, -7.38287643e-02],\n",
       "        [-9.25449505e-02,  3.06547172e-02],\n",
       "        [ 4.49647345e-02, -7.52891377e-02],\n",
       "        [ 8.06755126e-02, -6.69623688e-02],\n",
       "        [-2.49312213e-03, -5.39884297e-03],\n",
       "        [-9.83740389e-02, -7.14927688e-02],\n",
       "        [ 7.95990378e-02,  4.57155108e-02],\n",
       "        [-9.62979272e-02, -6.94943592e-02],\n",
       "        [-5.69977760e-02,  1.60330161e-02],\n",
       "        [ 5.24613559e-02, -6.21297173e-02],\n",
       "        [ 8.69001299e-02,  2.20278390e-02],\n",
       "        [ 7.21164122e-02, -7.39187449e-02],\n",
       "        [-3.03973425e-02,  4.95092943e-03],\n",
       "        [ 7.31532872e-02,  4.95579764e-02],\n",
       "        [-4.46930714e-02,  3.94465029e-02],\n",
       "        [ 5.35917617e-02, -9.22897905e-02],\n",
       "        [-4.41361666e-02, -9.07776356e-02],\n",
       "        [-2.57351846e-02, -5.76900691e-02],\n",
       "        [-5.93084171e-02, -2.15665437e-02],\n",
       "        [-3.30570969e-03, -2.10153144e-02],\n",
       "        [ 8.26693699e-03, -1.01646199e-03],\n",
       "        [ 8.99253413e-02,  5.56594916e-02],\n",
       "        [-8.91252831e-02,  6.56516701e-02],\n",
       "        [ 8.46657008e-02,  7.18598068e-02],\n",
       "        [ 4.82558645e-02,  4.26183734e-03],\n",
       "        [-8.87713507e-02,  2.55710306e-03],\n",
       "        [-1.50319077e-02, -4.54753451e-02],\n",
       "        [-1.08063817e-02,  5.32700820e-03],\n",
       "        [-2.57629976e-02, -3.57724838e-02],\n",
       "        [-5.97220622e-02,  3.77767533e-02],\n",
       "        [ 9.66798365e-02, -1.79539956e-02],\n",
       "        [-1.24164829e-02, -1.23880832e-02],\n",
       "        [-8.73340294e-02,  7.21550137e-02],\n",
       "        [-2.68115494e-02, -5.63318990e-02],\n",
       "        [-1.35671673e-02, -5.00812232e-02],\n",
       "        [-4.70573567e-02, -2.75789928e-02],\n",
       "        [-1.56682283e-02,  8.26712400e-02],\n",
       "        [ 9.83632542e-03,  6.41529039e-02],\n",
       "        [ 5.19098081e-02, -1.05251428e-02],\n",
       "        [ 5.96792102e-02, -1.12561220e-02],\n",
       "        [-6.02042340e-02,  6.92294165e-02],\n",
       "        [ 9.32736993e-02,  7.96639267e-03],\n",
       "        [ 1.24034761e-02,  1.54191069e-02],\n",
       "        [ 2.61299801e-03,  2.27637459e-02],\n",
       "        [-2.22181324e-02, -8.81181061e-02],\n",
       "        [-2.81176530e-02,  1.72399208e-02],\n",
       "        [ 7.42863268e-02, -6.58314452e-02],\n",
       "        [ 8.79743695e-02,  7.40072504e-02],\n",
       "        [-8.54900479e-02, -7.47435763e-02],\n",
       "        [-7.17528630e-03,  8.60640872e-03],\n",
       "        [-8.73028301e-03, -3.14721987e-02],\n",
       "        [-8.98595303e-02, -7.16425478e-02],\n",
       "        [-2.18320545e-02,  3.68533470e-02],\n",
       "        [-5.15620001e-02, -4.17277813e-02],\n",
       "        [-6.39324039e-02,  5.62938163e-03],\n",
       "        [ 2.74756886e-02,  8.68431628e-02],\n",
       "        [ 7.71007091e-02, -2.28194296e-02],\n",
       "        [-4.06942479e-02,  6.32442310e-02],\n",
       "        [ 6.82048574e-02, -7.21133724e-02],\n",
       "        [ 3.53622660e-02,  9.16295871e-02],\n",
       "        [-3.65478769e-02,  2.26714630e-02],\n",
       "        [ 6.78860247e-02,  1.28575051e-02],\n",
       "        [ 2.82214005e-02,  1.11276899e-02],\n",
       "        [ 4.68522944e-02, -3.66289616e-02],\n",
       "        [-1.06355865e-02, -2.80146860e-02],\n",
       "        [-1.44303031e-05, -2.22432725e-02],\n",
       "        [ 4.65421565e-02,  3.32734138e-02],\n",
       "        [ 4.12100665e-02,  3.57781090e-02],\n",
       "        [-9.34744552e-02, -9.62769240e-02],\n",
       "        [-6.02553524e-02, -5.43605313e-02],\n",
       "        [ 8.99663270e-02, -8.51256847e-02],\n",
       "        [-9.00700688e-02, -3.57673094e-02],\n",
       "        [-7.93173071e-03,  9.37115401e-03],\n",
       "        [-7.70217404e-02, -5.26726954e-02],\n",
       "        [-9.15462822e-02, -5.12446202e-02],\n",
       "        [-7.84944668e-02,  8.86421725e-02],\n",
       "        [-6.86227903e-02, -3.00843753e-02],\n",
       "        [-6.08426854e-02, -4.20616455e-02],\n",
       "        [-7.62950489e-03, -9.70892385e-02],\n",
       "        [-5.55127934e-02, -1.52428858e-02],\n",
       "        [-3.70705836e-02,  7.12225214e-02],\n",
       "        [ 1.39798662e-02, -8.72182623e-02],\n",
       "        [ 6.97802156e-02,  4.54970300e-02],\n",
       "        [-6.70164376e-02,  2.00642738e-02],\n",
       "        [ 9.82983783e-02,  6.91311359e-02],\n",
       "        [ 5.70602268e-02,  8.28305110e-02],\n",
       "        [-7.35885724e-02, -7.39588663e-02],\n",
       "        [-3.30891982e-02,  4.47337851e-02],\n",
       "        [-1.56481881e-02,  6.42411644e-03],\n",
       "        [-6.87458813e-02, -6.49137422e-02],\n",
       "        [-2.15844810e-03,  4.84592654e-02],\n",
       "        [-6.31857067e-02, -9.12499949e-02],\n",
       "        [ 3.46805006e-02, -5.24978861e-02],\n",
       "        [-3.09151243e-02,  5.56922108e-02],\n",
       "        [ 9.09013674e-02,  6.27895072e-02],\n",
       "        [ 1.05942646e-03, -9.15306807e-03],\n",
       "        [-2.61138938e-02,  4.47330549e-02],\n",
       "        [ 8.42191726e-02, -9.36787874e-02],\n",
       "        [ 1.43484930e-02, -3.85803357e-02],\n",
       "        [ 2.59281695e-02,  8.74179974e-02],\n",
       "        [ 7.48231588e-03, -8.17893296e-02],\n",
       "        [ 5.07704131e-02, -1.68567989e-02],\n",
       "        [ 2.11452134e-02,  4.05654348e-02],\n",
       "        [ 9.62262452e-02,  8.17242265e-02],\n",
       "        [ 9.94825438e-02,  5.08237481e-02],\n",
       "        [-9.28177685e-02,  9.19462815e-02],\n",
       "        [ 9.09027234e-02, -4.89757285e-02],\n",
       "        [-8.28927159e-02, -9.51210335e-02],\n",
       "        [ 8.34857821e-02,  5.10640107e-02],\n",
       "        [-1.94367729e-02,  5.76566905e-02],\n",
       "        [-6.96792156e-02, -7.28548169e-02],\n",
       "        [ 2.44630817e-02,  6.42418936e-02],\n",
       "        [ 5.94703965e-02, -7.86361918e-02],\n",
       "        [ 2.36680754e-03,  9.79928076e-02],\n",
       "        [-1.18553117e-02,  3.68747488e-02],\n",
       "        [-1.60741154e-02, -7.31994212e-03],\n",
       "        [ 4.95704934e-02, -6.93137199e-02],\n",
       "        [ 2.51558516e-02,  3.66956927e-02],\n",
       "        [ 8.77886638e-02, -2.45162938e-02],\n",
       "        [-9.49952453e-02,  6.68760240e-02],\n",
       "        [ 1.51759740e-02, -4.88272570e-02],\n",
       "        [ 1.42154275e-02, -6.65982664e-02],\n",
       "        [ 3.31903584e-02, -7.26848915e-02],\n",
       "        [ 1.99531838e-02,  2.17395201e-02],\n",
       "        [ 3.64906155e-02,  5.72027452e-02],\n",
       "        [-6.70065358e-02,  5.40596098e-02],\n",
       "        [ 9.32367817e-02, -5.72069660e-02],\n",
       "        [ 7.97920674e-02,  1.46451322e-02],\n",
       "        [ 6.86066151e-02, -3.24570872e-02],\n",
       "        [-3.92094702e-02,  7.37442672e-02],\n",
       "        [ 1.09631317e-02, -4.66254316e-02],\n",
       "        [-7.09361807e-02, -2.49535646e-02],\n",
       "        [ 4.04181555e-02, -3.37321535e-02],\n",
       "        [ 6.37823492e-02,  9.40736458e-02],\n",
       "        [ 6.98662400e-02, -9.91358235e-02],\n",
       "        [ 2.80716997e-02,  6.78777769e-02],\n",
       "        [ 6.66828081e-02, -8.80264714e-02],\n",
       "        [-8.77574757e-02, -7.15060830e-02],\n",
       "        [ 7.37069771e-02, -4.25846018e-02],\n",
       "        [ 9.43508893e-02, -9.89373252e-02],\n",
       "        [-9.27223563e-02,  3.60105596e-02],\n",
       "        [ 2.45591570e-02, -6.31151546e-04],\n",
       "        [ 3.44904475e-02,  4.93378490e-02],\n",
       "        [ 4.09509614e-03, -4.17847969e-02],\n",
       "        [-2.63802093e-02,  2.89711002e-02],\n",
       "        [ 4.19024825e-02,  3.15900482e-02],\n",
       "        [ 6.17552139e-02, -4.43163961e-02],\n",
       "        [ 4.40312736e-02, -6.11325763e-02],\n",
       "        [ 1.97758544e-02,  8.18242803e-02],\n",
       "        [ 3.72109525e-02, -8.94630775e-02],\n",
       "        [ 9.79801863e-02, -7.62815252e-02],\n",
       "        [-2.44506933e-02, -8.74134228e-02],\n",
       "        [ 2.87987245e-03,  8.87057632e-02],\n",
       "        [ 4.13230881e-02,  8.03527609e-02]], dtype=float32),\n",
       " 2: array([[0.06401023, 0.00357721],\n",
       "        [0.09798834, 0.02257733]], dtype=float32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(all_vocabs),embedding_dim=300,hidden_dim=[2], num_classes=2)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function to be used in the output layer. \n",
    "\n",
    "It takes as input `z` (array of real numbers) and returns `sig` (the softmax of `z`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))  \n",
    "    sig = e_z / np.sum(e_z, axis=0, keepdims=True)  \n",
    "    return sig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss that depends only on the true label `y` and the class probabilities vector `y_preds`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    y_preds = np.clip(y_preds, 1e-15, 1. - (1e-15))\n",
    "    loss = -np.sum(y * np.log(y_preds))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network \n",
    "(during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "  \n",
    "  relu_derivative($z_i$)=0, if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \n",
    "    a = np.maximum(0, z)\n",
    "    return a\n",
    "    \n",
    "def relu_derivative(z):\n",
    "    \n",
    "    dz = z.copy()\n",
    "    dz[dz <= 0] = 0\n",
    "    dz[dz > 0] = 1\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    \n",
    "\n",
    "    dropout_vec = np.random.rand(size) >  dropout_rate\n",
    "    return dropout_vec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.853632Z",
     "start_time": "2020-04-02T14:26:53.849944Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True False  True  True  True  True  True]\n",
      "[False  True  True  True  True  True  True  True False  True]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.5):\n",
    "    out_vals = {\"h_vecs\": [], \"a_vecs\": [], \"dropout_vecs\": []}\n",
    "\n",
    "    # Embedding lookup (average over input words)\n",
    "    emb_matrix = W[0]  # Shape: (vocab_size, embedding_dim)\n",
    "    emb_out = np.mean(emb_matrix[x], axis=0)  # Shape: (embedding_dim,)\n",
    "    out_vals[\"emb_out\"] = emb_out\n",
    "\n",
    "    # Initialize activations\n",
    "    a = emb_out.reshape(-1, 1)  # Shape: (embedding_dim, 1)\n",
    "\n",
    "    # Forward through hidden layers\n",
    "    for i in range(1, len(W) - 1):  # Hidden layers\n",
    "        h = W[i] @ a  # Linear transformation (h.shape: (hidden_dim, 1))\n",
    "        a = relu(h)  # Apply ReLU (a.shape: (hidden_dim, 1))\n",
    "\n",
    "        # Apply dropout\n",
    "        dropout_mask = (np.random.rand(*a.shape) > dropout_rate).astype(float)\n",
    "        a *= dropout_mask  # Dropout applied\n",
    "        a /= (1 - dropout_rate)  # Scale activations to maintain expected value\n",
    "\n",
    "        # Store values\n",
    "        out_vals[\"h_vecs\"].append(h)\n",
    "        out_vals[\"a_vecs\"].append(a)\n",
    "        out_vals[\"dropout_vecs\"].append(dropout_mask)\n",
    "\n",
    "    # Output layer (no activation, just softmax)\n",
    "    y_pred = W[len(W) - 1] @ a  # Shape: (num_classes, 1)\n",
    "    out_vals[\"y_pred\"] = y_pred.flatten()  # Convert to 1D array\n",
    "\n",
    "    return out_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:24:13.732705Z",
     "start_time": "2020-05-11T08:24:13.729741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, learning_rate=0.01, freeze_emb=False):\n",
    "    # Extract values from forward pass\n",
    "    h_vecs = out_vals[\"h_vecs\"]  # List of hidden layer outputs\n",
    "    a_vecs = out_vals[\"a_vecs\"]  # List of pre-activation values\n",
    "    dropout_vecs = out_vals[\"dropout_vecs\"]  # List of dropout masks\n",
    "    y_pred = out_vals[\"y_pred\"]  # Output layer logits\n",
    "    emb_out = out_vals[\"emb\"]  # Embedded input (averaged)\n",
    "\n",
    "    # Compute gradient at output layer\n",
    "    y_one_hot = np.zeros_like(y_pred)\n",
    "    y_one_hot[y - 1] = 1  # One-hot encode the true label\n",
    "    probs = y_pred  # logits are already in the form we need for cross-entropy\n",
    "    probs -= y_one_hot  # Cross-entropy gradient: (predicted - actual)\n",
    "\n",
    "    \n",
    "    # Gradient for last layer (hidden → output)\n",
    "    dW_last = np.outer(probs, h_vecs[len(h_vecs) - 1])\n",
    "    W[len(W) - 1] -= learning_rate * dW_last\n",
    "\n",
    "    # Backpropagation through hidden layers\n",
    "    delta = W[len(W) - 1].T @ probs  # Initial delta from output layer\n",
    "    for i in range(len(W) - 2, 0, -1):  # Iterate from last hidden layer to first\n",
    "        h = h_vecs[i - 1].flatten()  # fix index\n",
    "        drop = dropout_vecs[i - 1].flatten()  # fix index\n",
    "        delta *= (1 - np.tanh(h) ** 2)\n",
    "        delta *= drop\n",
    "\n",
    "        # Gradient for W[i] (previous layer → current layer)\n",
    "        a_prev = h_vecs[i - 2] if i > 1 else emb_out\n",
    "        dW = np.outer(delta, a_prev)\n",
    "        W[i] -= learning_rate * dW\n",
    "\n",
    "        # Backpropagate delta to next layer\n",
    "        delta = W[i].T @ delta\n",
    "\n",
    "    # Backpropagate into embedding layer\n",
    "    if not freeze_emb:\n",
    "        for idx in x:\n",
    "            W[0][idx] -= learning_rate * (delta / len(x))  # Average gradient\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.00001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, \n",
    "        print_progress=True):\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    prev_val_loss = float('inf')\n",
    "    \n",
    "    # Data Size\n",
    "    num_train = len(X_tr)\n",
    "\n",
    "   \n",
    "    for epoch in range(epochs):\n",
    "        epoch_training_loss = 0\n",
    "        # Learning for individual training from data\n",
    "        for i in range(num_train):\n",
    "            x = X_tr[i]  \n",
    "            y = Y_tr[i]  \n",
    "           \n",
    "            # forward\n",
    "            out_vals = forward_pass(x, W, dropout_rate=dropout)\n",
    "       \n",
    "            # loss\n",
    "            epoch_training_loss += categorical_loss(y, out_vals[\"y_pred\"])\n",
    "          \n",
    "            # backward\n",
    "            W = backward_pass(x, y, W, out_vals, lr, freeze_emb)\n",
    "\n",
    "        # Calculating average training loss per epoch\n",
    "        epoch_training_loss /= num_train\n",
    "        training_loss_history.append(epoch_training_loss)\n",
    "\n",
    "        # Verification Loss Calculation\n",
    "        val_loss = 0\n",
    "        for i in range(len(X_dev)):\n",
    "            x_dev = X_dev[i]\n",
    "            y_dev = Y_dev[i]\n",
    "            out_vals_dev = forward_pass(x_dev, W, dropout_rate=0)  \n",
    "            val_loss += categorical_loss(y_dev, out_vals_dev[\"y_pred\"])\n",
    "\n",
    "        val_loss /= len(X_dev)\n",
    "        validation_loss_history.append(val_loss)\n",
    "\n",
    "        # Training/validation loss output\n",
    "        if print_progress:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {epoch_training_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Training is terminated when the change in validation loss is less than tolerance.\n",
    "        if abs(prev_val_loss - val_loss) < tolerance:\n",
    "            print(f\"Training stopped early at epoch {epoch + 1} due to small validation loss change.\")\n",
    "            break\n",
    "\n",
    "        # Update previous relationships\n",
    "        prev_val_loss = val_loss\n",
    "\n",
    "    return W, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate your neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (61195, 300)\n",
      "Shape W1 (300, 256)\n",
      "Shape W2 (256, 128)\n",
      "Shape W3 (128, 64)\n",
      "Shape W4 (64, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 300 is different from 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W)):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape W\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i), W[i]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m W, loss_tr, dev_loss \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mX_dev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_dev_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mY_dev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_dev_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfreeze_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m plot_learning_process(loss_tr, dev_loss)\n",
      "Cell \u001b[0;32mIn[68], line 20\u001b[0m, in \u001b[0;36mSGD\u001b[0;34m(X_tr, Y_tr, W, X_dev, Y_dev, lr, dropout, epochs, tolerance, freeze_emb, print_progress)\u001b[0m\n\u001b[1;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m Y_tr[i]  \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m out_vals \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m epoch_training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m categorical_loss(y, out_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[66], line 25\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(x, W, dropout_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward through hidden layers\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(W) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Hidden layers\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m  \u001b[38;5;66;03m# Linear transformation (h.shape: (hidden_dim, 1))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     a \u001b[38;5;241m=\u001b[39m relu(h)  \u001b[38;5;66;03m# Apply ReLU (a.shape: (hidden_dim, 1))\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Apply dropout\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 300 is different from 256)"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "W = network_weights(vocab_size=len(all_vocabs),embedding_dim=300,\n",
    "                    hidden_dim=[256, 128, 64], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_train_indices, Y_train_array,\n",
    "                            W,\n",
    "                            X_dev=X_dev_indices, \n",
    "                            Y_dev=Y_dev_array,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.01,\n",
    "                            epochs=100)\n",
    "plot_learning_process(loss_tr, dev_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_process(training_losses, validation_losses):\n",
    "    \"\"\"\n",
    "    Plots the learning process by showing the training and validation loss over epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - training_losses: List of training losses at each epoch\n",
    "    - validation_losses: List of validation losses at each epoch\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(training_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, training_losses, label='Training Loss', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(epochs, validation_losses, label='Validation Loss', color='red', linestyle='-', marker='x')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.024444444444444446\n",
      "Precision: 0.1546352583586626\n",
      "Recall: 0.018333333333333333\n",
      "F1-Score: 0.03215459167752712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y_pred']) \n",
    "            for x,y in zip(X_test_indices,Y_test_array)]\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_test_array,preds_te))\n",
    "print('Precision:', precision_score(Y_test_array,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_test_array,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test_array,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary. Generally, that should work without any problem. If you get errors, you can modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in all_vocabs:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",all_word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (61195, 300)\n",
      "Shape W1 (500, 256)\n",
      "Shape W2 (256, 128)\n",
      "Shape W3 (128, 64)\n",
      "Shape W4 (64, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 500 is different from 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W)):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape W\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i), W[i]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m W, loss_tr, dev_loss \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mX_dev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_dev_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mY_dev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_dev_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfreeze_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m plot_learning_process(loss_tr, dev_loss)\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36mSGD\u001b[0;34m(X_tr, Y_tr, W, X_dev, Y_dev, lr, dropout, epochs, tolerance, freeze_emb, print_progress)\u001b[0m\n\u001b[1;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m Y_tr[i]  \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m out_vals \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m epoch_training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m categorical_loss(y, out_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(x, W, dropout_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Forward through hidden layers\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Hidden layers\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# 선형 계산\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     a_before_dropout \u001b[38;5;241m=\u001b[39m relu(h)  \u001b[38;5;66;03m# ReLU 적용\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     mask \u001b[38;5;241m=\u001b[39m dropout_mask(\u001b[38;5;28mlen\u001b[39m(a_before_dropout), dropout_rate)  \u001b[38;5;66;03m# 드롭아웃 마스크\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 500 is different from 300)"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(all_vocabs),embedding_dim=500,\n",
    "                    hidden_dim=[256, 128, 64], num_classes=3)\n",
    "W[0] = w_glove \n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_train_indices, Y_train_array,\n",
    "                            W,\n",
    "                            X_dev=X_dev_indices, \n",
    "                            Y_dev=Y_dev_array,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=True,\n",
    "                            tolerance=0.01,\n",
    "                            epochs=100)\n",
    "plot_learning_process(loss_tr, dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y_pred']) \n",
    "            for x,y in zip(X_test_indices,Y_test_array)]\n",
    "\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_test_array,preds_te))\n",
    "print('Precision:', precision_score(Y_test_array,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_test_array,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test_array,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures \n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. Do deeper architectures increase performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) \n",
    "            for x,y in zip(X_te,Y_te)]\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained)  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained) + X hidden layers    |   |   |   |   |\n",
    "\n",
    "\n",
    "Please discuss why your best performing model is better than the rest and provide a brief error analaysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
